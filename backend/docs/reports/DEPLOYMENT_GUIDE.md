# Production Deployment Guide

## ðŸš€ STEP 3: Deployment Verification (1-2 hours)

### Prerequisites
- Render.com account
- Perplexity API key
- Supabase database configured

### 1. Deploy to Render.com

#### Option A: Manual Deployment
1. Go to [Render.com](https://render.com)
2. Create new Web Service
3. Connect your GitHub repository
4. Use these settings:
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`
   - **Environment**: Python 3.9+

#### Option B: Using render.yaml
1. Push your code to GitHub
2. In Render dashboard, create "New Blueprint"
3. Connect repository and deploy using `docker/render.yaml`

### 2. Configure Environment Variables

Add these environment variables in Render dashboard:

```bash
JWS_SECRET=SJOYupb2v8rFU8nd3+B7G/5Y90BB+x0ihG+vTZ6M3lcAKnC0ThJtBEQvZz5ZgigQ+ZC96vAbmJQ0+1FMtLmqUw==
DATABASE_URL=postgresql://postgres.dusqpdhojbgfxwflukhc:Moto%40Roxy123@aws-1-ap-south-1.pooler.supabase.com:6543/postgres
POSTHOG_API_KEY=phc_lmGvuDZ7JiyjDmkL1T6Wy3TvDHgFdjt1zlH02fVziwU
SENTRY_DSN=https://0d595f5827bf2a4ae5da7d1ed1a09338@o4509949438328832.ingest.us.sentry.io/4510035576946688
JWT_SECRET_KEY=SJOYupb2v8rFU8nd3+B7G/5Y90BB+x0ihG+vTZ6M3lcAKnC0ThJtBEQvZz5ZgigQ+ZC96vAbmJQ0+1FMtLmqUw==
PERPLEXITY_API_KEY=your-perplexity-api-key
BHIV_LM_URL=https://api.perplexity.ai
BHIV_STORAGE_BACKEND=local
```

### 3. Configure LLM Service (Perplexity API)

```bash
python scripts/configure_llm_production.py
```

This script will:
- Validate your Perplexity API key
- Test storyboard generation
- Generate configuration report

### 4. Verify Deployment

```bash
python deploy_to_render.py
```

This script will:
- Test health endpoints
- Verify API functionality
- Test user authentication flow
- Monitor performance

### 5. Test with 5-10 Users

```bash
python scripts/production_test.py
```

This script will:
- Create 5-10 test users concurrently
- Test complete user workflows
- Upload content and submit feedback
- Generate test report

### 6. Monitor Production

```bash
python scripts/monitor_production.py
```

This script will:
- Monitor application health
- Track performance metrics
- Check database connectivity
- Generate monitoring reports

## ðŸ“Š Expected Results

### Deployment Success Criteria
- âœ… Health endpoint returns 200
- âœ… API endpoints respond correctly
- âœ… User authentication works
- âœ… Database connectivity confirmed
- âœ… 90%+ uptime during monitoring

### User Testing Success Criteria
- âœ… 80%+ user registration success
- âœ… 70%+ content upload success
- âœ… 60%+ feedback submission success
- âœ… Response times < 3 seconds

### Performance Benchmarks
- âœ… Average response time < 2 seconds
- âœ… Error rate < 5%
- âœ… Concurrent user support (5-10 users)
- âœ… Database query performance

## ðŸ”§ Troubleshooting

### Common Issues

1. **Database Connection Errors**
   - Verify Supabase credentials
   - Check IPv4 connection string
   - Test with `python test_db_connection.py`

2. **LLM Service Errors**
   - Validate Perplexity API key
   - Check API rate limits
   - Test with `python scripts/configure_llm_production.py`

3. **Deployment Failures**
   - Check build logs in Render dashboard
   - Verify requirements.txt dependencies
   - Check environment variables

4. **Performance Issues**
   - Monitor with `python scripts/monitor_production.py`
   - Check database query performance
   - Review error logs

## ðŸ“‹ Deployment Checklist

- [ ] Render.com service created
- [ ] Environment variables configured
- [ ] Perplexity API key validated
- [ ] Health check passes
- [ ] API endpoints tested
- [ ] User workflow verified
- [ ] 5-10 user test completed
- [ ] Performance monitoring active
- [ ] Error tracking configured
- [ ] Deployment report generated

## ðŸŽ‰ Success Confirmation

Your deployment is successful when:
1. All scripts run without errors
2. Test reports show >80% success rates
3. Monitoring shows stable performance
4. Users can complete full workflows

## ðŸ“ž Support

If you encounter issues:
1. Check the generated reports in project directory
2. Review Render.com deployment logs
3. Test individual components with provided scripts
4. Verify all environment variables are set correctly